{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKPYxsCuUBbm"
      },
      "source": [
        "# **Stanford 3D Indoor Scene Dataset (S3DIS)**\n",
        "\n",
        "This notebook explores the [S3DIS](https://svl.stanford.edu/assets/papers/3D_Semantic_Parsing.pdf) dataset using the S3DIS data folder (which is about 30GB). It will show how to preprocess the data and store it in a reduced format with and without partitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAIXJdDII92E"
      },
      "source": [
        "Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omMUEoaEHqS2"
      },
      "outputs": [],
      "source": [
        "# !pip install open3d==0.16.0 # must be at least 0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD3z90hjUAir",
        "outputId": "603362a3-d322-472c-f411-524c14d27606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
            "[Open3D INFO] WebRTC GUI backend enabled.\n",
            "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
            "[Open3D INFO] Resetting default logger to print to terminal.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import open3d as o3\n",
        "# from open3d import JVisualizer # For Colab Visualization\n",
        "from open3d.web_visualizer import draw # for non Colab\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moxHa_UJI7go"
      },
      "source": [
        "Get Dataset path and save path for new dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT = r'C:\\Users\\itber\\Documents\\datasets\\S3DIS\\Stanford3dDataset_v1.2_Aligned_Version'\n",
        "SAVE_PATH = r'C:\\Users\\itber\\Documents\\datasets\\S3DIS\\Stanford3dDataset_v1.2_Reduced_Aligned_Version'\n",
        "PARTITION_SAVE_PATH = r'C:\\Users\\itber\\Documents\\datasets\\S3DIS\\Stanford3dDataset_v1.2_Reduced_Partitioned_Aligned_Version'\n",
        "\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.mkdir(SAVE_PATH)\n",
        "\n",
        "if not os.path.exists(PARTITION_SAVE_PATH):\n",
        "    os.mkdir(PARTITION_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helpers for categories and colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "CATEGORIES = {\n",
        "    'ceiling'  : 0, \n",
        "    'floor'    : 1, \n",
        "    'wall'     : 2, \n",
        "    'beam'     : 3, \n",
        "    'column'   : 4, \n",
        "    'window'   : 5,\n",
        "    'door'     : 6, \n",
        "    'table'    : 7, \n",
        "    'chair'    : 8, \n",
        "    'sofa'     : 9, \n",
        "    'bookcase' : 10, \n",
        "    'board'    : 11,\n",
        "    'stairs'   : 12,\n",
        "    'clutter'  : 13\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unique color map generated via\n",
        "# https://mokole.com/palette.html\n",
        "COLOR_MAP = {\n",
        "    0  : (47, 79, 79),    # ceiling - darkslategray\n",
        "    1  : (139, 69, 19),   # floor - saddlebrown\n",
        "    2  : (34, 139, 34),   # wall - forestgreen\n",
        "    3  : (75, 0, 130),    # beam - indigo\n",
        "    4  : (255, 0, 0),     # column - red \n",
        "    5  : (255, 255, 0),   # window - yellow\n",
        "    6  : (0, 255, 0),     # door - lime\n",
        "    7  : (0, 255, 255),   # table - aqua\n",
        "    8  : (0, 0, 255),     # chair - blue\n",
        "    9  : (255, 0, 255),   # sofa - fuchsia\n",
        "    10 : (238, 232, 170), # bookcase - palegoldenrod\n",
        "    11 : (100, 149, 237), # board - cornflower\n",
        "    12 : (255, 105, 180), # stairs - hotpink\n",
        "    13 : (0, 0, 0)        # clutter - black\n",
        "}\n",
        "\n",
        "map_colors = lambda x : COLOR_MAP[x]\n",
        "v_map_colors = np.vectorize(map_colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtain a Dict with all spaces and paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "area_nums = '1-6' # decide on the number of areas to obtain\n",
        "area_dict = {}\n",
        "\n",
        "# get areas based on split\n",
        "areas = glob(os.path.join(ROOT, f'Area_[{area_nums}]*'))\n",
        "\n",
        "for area in areas:\n",
        "    # get all subfolders in area (corresponds to disjoint spaces (or locations))\n",
        "    spaces = next(os.walk(area))[1]\n",
        "\n",
        "    # get dict to store spaces\n",
        "    space_dict = {}\n",
        "\n",
        "    # for each space\n",
        "    for space in spaces:\n",
        "        space = os.path.join(area, space)\n",
        "        annotations = os.path.join(space, 'Annotations')\n",
        "\n",
        "        # get individual segmentation filepaths\n",
        "        segments = glob(os.path.join(annotations, '*.txt'))\n",
        "        \n",
        "        # update space dict\n",
        "        space_dict.update({space.split('\\\\')[-1] : segments})\n",
        "\n",
        "    # update area dict\n",
        "    area_dict.update({area.split('\\\\')[-1] : space_dict})\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to obtain space data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_space_data(space_segments, categories=CATEGORIES):\n",
        "    ''' Obtains space data in (x,y,z),cat format all types are float32 \n",
        "        Inputs: \n",
        "            space_segments - (list) filepaths to all annotaed space segments \n",
        "                            for the current space.\n",
        "                            e.g. area_dict['Area_1']['conferenceRoom_2']\n",
        "            categories - (dict) maps string category to numeric category\n",
        "        Outputs:\n",
        "            space_data - (array) \n",
        "        '''\n",
        "    # space data list (x,y,z, cat)\n",
        "    space_data = []\n",
        "    for seg_path in space_segments:\n",
        "\n",
        "        # get truth category and xyz points\n",
        "        cat = CATEGORIES[seg_path.split('\\\\')[-1].split('_')[0]]\n",
        "        xyz = pd.read_csv(seg_path, header=None, sep=' ', \n",
        "                          dtype=np.float32, usecols=[0,1,2]).to_numpy()\n",
        "\n",
        "        # add truth to xyz points and add to space list\n",
        "        space_data.append(np.hstack((xyz, \n",
        "                                     np.tile(cat, (len(xyz), 1)) \\\n",
        "                                     .astype(np.float32))))\n",
        "\n",
        "    # combine into single array and return\n",
        "    return np.vstack(space_data)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now obtain reduced data format with no paritions (unsliced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107.52749156951904\n"
          ]
        }
      ],
      "source": [
        "tic = time.time()\n",
        "\n",
        "for area in area_dict:\n",
        "    # create new directory\n",
        "    save_dir = os.path.join(SAVE_PATH, area)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "\n",
        "    for space in area_dict[area]:\n",
        "        # obtain xyz points with truth labels\n",
        "        space_data = pd.DataFrame(get_space_data(area_dict[area][space]))\n",
        "        \n",
        "        # save as .hdf5 file in new directory\n",
        "        save_path = os.path.join(save_dir, space + '.hdf5')\n",
        "        space_data.to_hdf(save_path, key='space_data')\n",
        "\n",
        "\n",
        "toc = time.time()\n",
        "print(toc - tic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# space_data = pd.read_hdf(os.path.join(save_dir, space + '.hdf5'), key='space_data').to_numpy()\n",
        "space_data = get_space_data(area_dict['Area_3']['conferenceRoom_1'], categories=CATEGORIES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display basic point cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0881579ab4c14ac888ec757dad4098af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "WebVisualizer(window_uid='window_0')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pcd = o3.geometry.PointCloud()\n",
        "pcd.points = o3.utility.Vector3dVector(space_data[:,:3])\n",
        "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(space_data[:, 3])).T/255)\n",
        "\n",
        "draw(pcd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find out how to partition each point cloud into 1x1meter or just smaller chunks as in PointNet\n",
        "\n",
        "Here are the basic steps\n",
        "\n",
        "1) Shift the point cloud such that the min value is 0 for all axes\n",
        "2) Find the max of the shifted x and y dimensions to determine how many partitions to create\n",
        "3) Get x and y bins using np.histogram()\n",
        "4) Use the bins to get (x,y) space partitions\n",
        "5) Use the partitions to slice the space in the (x,y) dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or we could try the Point Net Approach and shift the min of the data to the origin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partition functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_slice(points, xyz_s, xpart, ypart):\n",
        "    ''' Obtains Point Cloud Slices from the (x,y) partitions \n",
        "        By default this will obtain roughly 1x1 partitions\n",
        "        inputs:\n",
        "            points - (array) could be xyz, rgb or any input array\n",
        "            xyz_s - (Nx3 array) 0 min shifter point cloud array \n",
        "            xpart - xpartitions [[lower, upper]]\n",
        "            ypart - ypartitions [[lower, upper]]\n",
        "        '''\n",
        "    x_slice = (xyz_s[:, 0] >= xpart[0]) \\\n",
        "              & (xyz_s[:, 0] <= xpart[1])\n",
        "\n",
        "    y_slice = (xyz_s[:, 1] >= ypart[0]) \\\n",
        "              & (xyz_s[:, 1] <= ypart[1])\n",
        "    \n",
        "    return points[x_slice & y_slice, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_partitions(xyz, xyz_s, c=1.):\n",
        "    ''' Obtains Point Cloud Space Partitions\n",
        "        Inputs:\n",
        "            xyz_s - (Nx3 array) 0 min shifted point cloud array \n",
        "            c - (float) factor for deciding how many partitions to create (larger --> less partitions)\n",
        "        Outputs: \n",
        "            partitions - (tuple) x and y parition arrays with \n",
        "                         format: [[lower, upper]]\n",
        "        '''\n",
        "    ## get number of x, y bins\n",
        "    range_ = np.abs(xyz.max(axis=0) - xyz.min(axis=0))\n",
        "    num_xbins, num_ybins, _ = np.uint8(np.round(range_ / c))\n",
        "\n",
        "    # uncomment this to generate ~1x1m partitions\n",
        "    # num_xbins, num_ybins, _ = np.uint8(np.ceil(np.max(xyz_s, 0)))\n",
        "\n",
        "    ## get x, y bins\n",
        "    _, xbins = np.histogram(xyz_s[:, 0], bins=num_xbins)\n",
        "    _, ybins = np.histogram(xyz_s[:, 1], bins=num_ybins)\n",
        "\n",
        "    ## get x y space paritions\n",
        "    x_parts = np.vstack((xbins[:-1], xbins[1:])).T\n",
        "    y_parts = np.vstack((ybins[:-1], ybins[1:])).T\n",
        "\n",
        "    return x_parts, y_parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "146.38069939613342\n"
          ]
        }
      ],
      "source": [
        "tic = time.time()\n",
        "\n",
        "num_invalid_partitions = 0\n",
        "\n",
        "for area in area_dict:\n",
        "    # create new directory\n",
        "    save_dir = os.path.join(PARTITION_SAVE_PATH, area)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "\n",
        "    for space in area_dict[area]:\n",
        "        # obtain xyz points with truth labels\n",
        "        space_data = get_space_data(area_dict[area][space])\n",
        "\n",
        "        # obtain x, y partitions\n",
        "        xyz = space_data[:, :3]\n",
        "\n",
        "        # get 0 min shifted points\n",
        "        xyz_s = xyz - xyz.min(axis=0)\n",
        "        x_parts, y_parts = get_partitions(xyz, xyz_s, c=1.5)\n",
        "\n",
        "        # counter for parition saving\n",
        "        i = 0\n",
        "        for x_part in x_parts:\n",
        "            for y_part in y_parts:\n",
        "                \n",
        "                space_slice = pd.DataFrame(get_slice(space_data, xyz_s, x_part, y_part))\n",
        "\n",
        "                # only save if partition has at least 100 points:\n",
        "                if len(space_slice) > 100:\n",
        "                    i += 1\n",
        "                    save_path = os.path.join(save_dir, space + f'_partition{i}_.hdf5')\n",
        "                    space_slice.to_hdf(save_path, key='space_slice')\n",
        "                else:\n",
        "                    num_invalid_partitions += 1\n",
        "\n",
        "\n",
        "toc = time.time()\n",
        "print(toc - tic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-10.377,  38.873,   2.54 ,   3.   ],\n",
              "       [-10.387,  38.875,   2.544,   3.   ],\n",
              "       [-10.523,  38.782,   2.433,   3.   ],\n",
              "       ...,\n",
              "       [ -9.831,  39.247,   1.533,   2.   ],\n",
              "       [ -9.833,  39.247,   1.637,   2.   ],\n",
              "       [ -9.833,  39.247,   1.691,   2.   ]], dtype=float32)"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "space_data = pd.read_hdf(save_path, key='space_slice').to_numpy()\n",
        "space_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83c5d47f07194a00a0aa173fb2046a9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "WebVisualizer(window_uid='window_35')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pcd = o3.geometry.PointCloud()\n",
        "pcd.points = o3.utility.Vector3dVector(space_data[:, :3])\n",
        "pcd.colors = o3.utility.Vector3dVector(np.vstack(v_map_colors(space_data[:, 3])).T/255)\n",
        "\n",
        "draw(pcd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMu+KmrIdBiqaSv6b1/RbGv",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c3515861ec4313dacaa20b0eec5bf326e6557b6589b7b6a4fe3c8baa566747d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
